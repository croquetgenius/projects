{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing & Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we read in the datasets collected in notebook 01 and format our text data (i.e. tweets). This includes removing stopwords, lemmatization, removing emails/urls, and making all text lowercase (this was all done with Spacy). Next, we used Vader Sentiment to analyze our data and produce a polarity rating. We chose Vader Sentiment because draft versions of our code using Text Blob seemed to perform more poorly and because Vader features a compound score, which is a 'normalized, weighted composite score' (quoted from the Vader documentation linked in the readme). Polarity is assigned to both raw and processed text for potential comparison.\n",
    "\n",
    "Additionally, columns are further formatted/organized here, and each dataset is exported for data visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key \"text.kerning_factor\" on line 4 in\n",
      "/Users/jonathanbenton/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test_patch.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.1.3/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "from datetime import date, timedelta\n",
    "import os\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 397195 entries, 0 to 397194\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   text    397195 non-null  object\n",
      " 1   date    397195 non-null  object\n",
      " 2   city    397195 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 9.1+ MB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 396963 entries, 0 to 396962\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   text    396963 non-null  object\n",
      " 1   date    396963 non-null  object\n",
      " 2   city    396963 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 9.1+ MB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 37658 entries, 0 to 37657\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    37658 non-null  object\n",
      " 1   date    37658 non-null  object\n",
      " 2   city    37658 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 882.7+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Reads in dataframes from notebook 01 and concatenates subsets into master datasets\n",
    "tweets_2020 = pd.concat([pd.read_csv('../data/tweets/raw_top10_cities_2020.csv'), pd.read_csv('../data/tweets/raw_bottom10_cities_2020.csv')],\n",
    "                        ignore_index=True)\n",
    "tweets_2019 = pd.concat([pd.read_csv('../data/tweets/raw_top10_cities_2019.csv'), pd.read_csv('../data/tweets/raw_bottom10_cities_2019.csv')],\n",
    "                        ignore_index=True)\n",
    "keywords_tweets = pd.concat([pd.read_csv('../data/tweets/raw_top5_cities_keywords.csv'), pd.read_csv('../data/tweets/raw_bottom5_cities_keywords.csv')],\n",
    "                        ignore_index=True)\n",
    "\n",
    "# Checks dtypes and nulls\n",
    "print(tweets_2020.info())\n",
    "print(tweets_2019.info())\n",
    "print(keywords_tweets.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COVID-19 Word Frequency\n",
    "Here we searched through our 2020 tweets to see which words in a large list of COVID-19-related words appear most in our tweets. From that list, we picked seven that we felt were most relevant to our project and repulled tweets from our chosen cities that contained any of those words (this is seen in notebook 01 under the \"Tweets By Keywords 2020\" header). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('quarantine', 1791),\n",
       " ('social', 1652),\n",
       " ('virus', 1123),\n",
       " ('pandemic', 954),\n",
       " ('mask', 886),\n",
       " ('COVID-19', 826),\n",
       " ('distancing', 693),\n",
       " ('lockdown', 690),\n",
       " ('coronavirus', 635),\n",
       " ('China', 575),\n",
       " ('COVID', 327),\n",
       " ('Social', 313),\n",
       " ('Covid', 286),\n",
       " ('corona', 271),\n",
       " ('covid', 248),\n",
       " ('Covid-19', 233),\n",
       " ('Pandemic', 130),\n",
       " ('COVID19', 119),\n",
       " ('covid-19', 107),\n",
       " ('outbreak', 92)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covid_keywords = ['coronavirus', 'koronavirus', 'corona', 'virus', 'covid', 'covid-19', 'Covid', 'Covid-19', \n",
    "                  'COVID', 'COVID-19', 'covid19', 'COVID19', 'Pandemic', 'pandemic', 'Epidemic', 'epidemic', \n",
    "                  'outbreak', 'Outbreak', 'China', 'china', 'covd', 'COVD', 'coronapocalypse', 'coronials', \n",
    "                  'Coronials', 'social', 'distancing', 'Social', 'Distancing', 'socialdistancing', 'panicbuy', \n",
    "                  'panic buy', 'panic buying' 'panicbuying', 'quarantine', 'quarantining', 'lock down', \n",
    "                  'lockdown', 'chinese', 'virus', 'chinesevirus', 'trumppandemic', 'trumpandemic', \n",
    "                  'trump pandemic', 'Trump pandemic', 'flattenthecurve', 'flatten the curve', 'quarantinelife', \n",
    "                  'quarentine', 'stayathome', 'stay at home', 'stayhome', 'stay home','StayHome', 'withme', \n",
    "                  'quarantineandchill', 'mask']\n",
    "\n",
    " # tokenize\n",
    "biglist = [i.split(\" \") for i in tweets_2020.text]\n",
    "# recursive unpacking of nested lists\n",
    "biglist = [item for items in biglist for item in items]\n",
    "# filtering for covid kewords\n",
    "counts = [word for word in biglist if word in covid_keywords]\n",
    "# making a counter object from filtered list\n",
    "c = Counter(counts)\n",
    "# inspecting top 20\n",
    "c.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonathanbenton/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if sys.path[0] == '':\n",
      "/Users/jonathanbenton/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "\n",
      "2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creates a for loop that processes the text data \n",
    "df_list = [tweets_2020, tweets_2019, keywords_tweets] \n",
    "for i, df in enumerate(df_list):\n",
    "    print(i) # enumerate/printing here allowed us to track progress, as this code ran for many hours\n",
    "    print()\n",
    "    df['time'] = pd.to_datetime(df['date'], utc=True).dt.time\n",
    "    df['date'] = pd.to_datetime(df['date'], utc=True).dt.date\n",
    "    df = df[['text', 'date', 'time', 'city']]\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    df['lemmata'] = [[token.lemma_ for token in doc if token.lemma_ != '-PRON-' \\\n",
    "                           if token.is_alpha if not token.is_punct if not token.is_stop \\\n",
    "                           if not token.like_url if not token.like_email] \\\n",
    "                           for doc in [nlp(i) for i in df.text]]\n",
    "    # lower the lemmata\n",
    "    df['lemmata'] = [[token.lower() for token in doc] for doc in df.lemmata]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses Vader to analyze both raw tweets and cleaned tweets\n",
    "for df in df_list:\n",
    "    df['text_polarity'] = df['text'].apply(lambda text: pd.Series(SentimentIntensityAnalyzer().polarity_scores(text)['compound']))\n",
    "    df['lemmata_polarity'] = df['lemmata'].apply(lambda text: pd.Series(SentimentIntensityAnalyzer().polarity_scores(text)['compound']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates top_10 flag for visualization notebook\n",
    "top_10 = ['New York City', 'Washington, D.C.', 'Anchorage',\n",
    "       'Honolulu', 'Newark', 'Providence', 'Seattle', 'Boston',\n",
    "       'Manchester', 'Charleston']\n",
    "for df in df_list:\n",
    "    df['top_10'] = np.where(df['city'].isin(top_10), 1, 0)\n",
    "    df = df[['city', 'top_10', 'date', 'time', 'text', 'text_polarity', 'lemmata', 'lemmata_polarity']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(396963, 8)\n",
      "(397195, 8)\n",
      "(37658, 8)\n",
      "\n",
      "(368761, 8)\n",
      "(367369, 8)\n",
      "(23803, 8)\n"
     ]
    }
   ],
   "source": [
    "# Checks the difference in data after duplicates are removed\n",
    "print(tweets_2019.shape)\n",
    "print(tweets_2020.shape)\n",
    "print(tweets_keywords.shape)\n",
    "print()\n",
    "print(tweets_2019.drop_duplicates(subset='text').shape)\n",
    "print(tweets_2020.drop_duplicates(subset='text').shape)\n",
    "print(tweets_keywords.drop_duplicates(subset='text').shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drops duplicate tweets and saves cleaned tweets w/sentiment analysis as csv for data viz\n",
    "tweets_2019.drop_duplicates(subset='text').to_csv('../data/tweets/tweets_2019_polarity.csv', index=False)\n",
    "tweets_2020.drop_duplicates(subset='text').to_csv('../data/tweets/tweets_2020_polarity.csv', index=False)\n",
    "tweets_keywords.drop_duplicates(subset='text').to_csv('../data/tweets/tweets_keywords_polarity.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
